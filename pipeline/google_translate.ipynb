{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate google translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path.cwd().parent\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from google.cloud import translate_v2 as gg_translate\n",
    "import pandas as pd\n",
    "import os\n",
    "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = YOUR_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('CWQ Annotation - EN.csv')\n",
    "sentences = list(sentences['questionWithBrackets'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_client = gg_translate.Client()\n",
    "def translate_with_qmark(q, target):\n",
    "    q = q if q[-1] == \"?\" else q + \"?\"\n",
    "    q_trans = translate_client.translate(q, target_language=target)[\"translatedText\"]\n",
    "    return q_trans[:-1] if q_trans[-1] == \"?\" else q_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = []\n",
    "for en in sentences:\n",
    "    jp.append(translate_with_qmark(en, 'ja'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = [{'questionWithBrackets':en, 'questionWithBrackets_jp_gg':jp} for en,jp in zip(sentences, jp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a file for manual assessment\n",
    "import csv\n",
    "with open('CWQ Annotation - JP - Google.csv','w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['questionWithBrackets', 'questionWithBrackets_jp_gg'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference-based assessment\n",
    "from sacrebleu.metrics import BLEU\n",
    "from sacrebleu.tokenizers import tokenizer_ja_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('CWQ Annotation - GOLD-JP.csv')\n",
    "sentences_en = list(sentences['questionPatternModEntities'].dropna())\n",
    "gold_jp = list(sentences['questionPatternModEntitiesJP'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_jp = []\n",
    "for en in sentences_en:\n",
    "    infer_jp.append(translate_with_qmark(en, 'ja'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 42.36 76.9/53.7/38.4/26.8 (BP = 0.933 ratio = 0.935 hyp_len = 3266 ref_len = 3494)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = BLEU(tokenize='ja-mecab')\n",
    "bleu.corpus_score(hypotheses=infer_jp, references=[gold_jp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU = 42.36 76.9/53.7/38.4/26.8 (BP = 0.933 ratio = 0.935 hyp_len = 3266 ref_len = 3494)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep(sen):\n",
    "    sen = sen.replace('だった','')\n",
    "    sen = sen.replace('？','')\n",
    "    sen = sen.replace('でしたか','')\n",
    "    sen = sen.replace('ですか','')\n",
    "    sen = sen.replace('によって','に')\n",
    "    sen = sen.replace(' ','')\n",
    "    return sen\n",
    "new_infer_jp = []\n",
    "new_gold_jp = []\n",
    "for inf, gol in zip(infer_jp, gold_jp):\n",
    "    new_infer_jp.append(rep(inf))\n",
    "    new_gold_jp.append(rep(gol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 45.07 79.0/56.7/41.4/29.3 (BP = 0.934 ratio = 0.936 hyp_len = 3102 ref_len = 3315)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = BLEU(tokenize='ja-mecab')\n",
    "bleu.corpus_score(hypotheses=new_infer_jp, references=[new_gold_jp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('p1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45d505a4b3ff2502d4f9cd4a15623bf9e22048bba70d0307a35b1db3cbec819a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
